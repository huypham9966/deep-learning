{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%load_ext autoreload\n",
    "%autoreload \n",
    "#import sys\n",
    "#for p in sys.path:\n",
    "#    print(p)\n",
    "#!jupyter kernelspec list\n",
    "#!pip install networkx\n",
    "#!pip install node2vec\n",
    "import numpy as np\n",
    "import math as math \n",
    "import tensorflow as tf\n",
    "import graph_synthesis as gs\n",
    "from random_mini_batches import random_mini_batches_perm\n",
    "from random_mini_batches import random_mini_batches2\n",
    "import networkx as nx\n",
    "import node2vec as n2v\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullNN:\n",
    "    \n",
    "    def __init__(self, inputdat, labeldat, testdat, testlabeldat, layer_dims, decayrate = 0.995,\n",
    "                 l2reg = 1e-3, learning_rate = 0.008, batch_size = 256, epoch_nums = 20000):\n",
    "        \n",
    "        #Data\n",
    "        self.inputdat = inputdat\n",
    "        self.labeldat = labeldat\n",
    "        self.testdat = testdat\n",
    "        self.testlabeldat = testlabeldat\n",
    "        self.num_eg = inputdat.shape[0]\n",
    "        \n",
    "        #Hyperparameters\n",
    "        self.layer_dims = layer_dims\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch_nums = epoch_nums\n",
    "        self.l2reg = l2reg  \n",
    "        self.decayrate = decayrate \n",
    "        \n",
    "        #Initialize\n",
    "        tf.reset_default_graph()\n",
    "        self.initdata()\n",
    "        \n",
    "        #Create graph\n",
    "        self.output = self.model()\n",
    "        self.optimizer, self.cost = self.train()\n",
    "        self.prediction, self.accuracy = self.test()\n",
    "        self.summary_op = self.create_summaries()\n",
    "        \n",
    "        #Run Tensorflow\n",
    "        config = tf.ConfigProto(allow_soft_placement = True, log_device_placement = True, device_count = {'GPU': 1})\n",
    "        self.sess = tf.Session(config = config)\n",
    "        init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        self.sess.run(init)\n",
    "        \n",
    "        #Tensorboard setup\n",
    "        self.writer = tf.summary.FileWriter('./FullNNSave',  (self.sess).graph)\n",
    "        \n",
    "        #Train model \n",
    "        self.graph_classifier()\n",
    "        self.prediction_num, self.error_num = self.evaluate(\"train\") \n",
    "        \n",
    "    def  create_summaries(self):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar(\"loss\", self.cost)\n",
    "            tf.summary.scalar(\"accuracy\", self.accuracy)\n",
    "            tf.summary.histogram(\"histogram_loss\", self.cost)\n",
    "            summary_op = tf.summary.merge_all()\n",
    "        return summary_op\n",
    "    \n",
    "    def initdata(self):\n",
    "        self.tflearnrate = tf.placeholder(tf.float32, name = \"learnrate\")\n",
    "        self.mode = tf.placeholder(tf.bool, name = \"mode\")\n",
    "        self.input = tf.placeholder(tf.float32, shape = [None, self.layer_dims[0]], name = \"input\")\n",
    "        self.label = tf.placeholder(tf.float32, shape = [None, 1], name = \"label\")\n",
    "\n",
    "    \n",
    "    def model(self):\n",
    "        layer_dims = self.layer_dims\n",
    "        layers = {}\n",
    "        layers[0] = self.input\n",
    "        for i in range(len(layer_dims)-1):\n",
    "            if (i < len(layer_dims) - 2):\n",
    "                scope = 'layers' + str(i+1)\n",
    "                with tf.variable_scope(scope):\n",
    "                    #layers[i+1] = tf.layers.dense(units = layer_dims[i+1], activation = tf.nn.relu, inputs = layers[i])\n",
    "                    layers[i+1] = tf.contrib.layers.fully_connected(num_outputs = layer_dims[i+1], \\\n",
    "                                                        activation_fn = None, inputs = layers[i], scope = 'dense')\n",
    "                    layers[i+1] = tf.contrib.layers.batch_norm(layers[i+1], is_training = self.mode, scope = 'batchnorm')\n",
    "                    layers[i+1] = tf.nn.relu(layers[i+1], 'relu')\n",
    "            else:\n",
    "                scope = 'layers' + str(i+1)\n",
    "                with tf.variable_scope(scope):\n",
    "                    layers[len(layer_dims)-1] = tf.layers.dense(units = layer_dims[len(layer_dims)-1], \\\n",
    "                                                    activation = tf.nn.sigmoid, inputs = layers[len(layer_dims)-2])\n",
    "        \n",
    "        output = layers[len(layer_dims)-1]\n",
    "        return output \n",
    "    \n",
    "    def train(self):\n",
    "        #Saver for data \n",
    "        self.saver = tf.train.Saver()\n",
    "        self.global_step = tf.Variable(0, dtype = tf.int32, trainable = False, name = 'global_step')\n",
    "        \n",
    "        #Define cost \n",
    "        reg_loss = self.l2reg * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()) \n",
    "        cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = self.output, labels = self.label)) + \\\n",
    "                                                                                                            reg_loss \n",
    "        \n",
    "        #Batch Normalization\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.tflearnrate).minimize(cost, global_step = self.global_step)\n",
    "            \n",
    "        return optimizer, cost \n",
    "    \n",
    "    def test(self):\n",
    "        prediction = tf.greater(self.output, tf.constant(0.5, dtype = tf.float32))\n",
    "        with tf.name_scope('accuracy'):\n",
    "            with tf.name_scope('correct_prediction'):\n",
    "                correct_prediction = tf.equal(tf.cast(prediction, dtype = tf.float32), self.label)\n",
    "            with tf.name_scope('accuracy'):\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "                tf.summary.scalar('accuracy', accuracy)\n",
    "        return prediction, accuracy\n",
    "    \n",
    "    def graph_classifier(self):\n",
    "        curlearnrate = self.learning_rate\n",
    "        step = 0 \n",
    "        for epochs in range(self.epoch_nums):\n",
    "            output = []\n",
    "            mini_batch = random_mini_batches_perm(self.inputdat, self.labeldat, self.batch_size)\n",
    "            curlearnrate = curlearnrate * self.decayrate \n",
    "            for dat in mini_batch:\n",
    "                step = step + 1 \n",
    "                _, cost, output, summary = self.sess.run([self.optimizer, self.cost, self.output, self.summary_op], \n",
    "                                                feed_dict = {self.input: dat[0], self.label: dat[1], \\\n",
    "                                                             self.mode: True, self.tflearnrate: curlearnrate})\n",
    "                (self.writer).add_summary(summary, global_step = step)\n",
    "            if ((epochs + 1) % 500 == 0):\n",
    "                _, errortrain = self.evaluate(\"train\")\n",
    "                cost, accuracy = self.sess.run([self.cost, self.accuracy], \\\n",
    "                        feed_dict = {self.input: self.inputdat, self.label: self.labeldat, \\\n",
    "                                     self.mode: False, self.tflearnrate: curlearnrate})\n",
    "                (self.saver).save(self.sess, 'checkpoints/CNN', global_step=self.global_step)\n",
    "                _, errortest = self.evaluate(\"test\")\n",
    "                print(\"epoch\", (epochs+1), \"    |    \", \"cost\", \"%.6e\" % cost, \"    |    \", \n",
    "                      \"error train\", \"%.3f\" % (errortrain * 100), \"    |    \", \"error test\", \"%.3f\" % (errortest * 100))\n",
    "        \n",
    "    def evaluate(self, mode):\n",
    "        #Get predictions\n",
    "        if mode == \"train\":\n",
    "            predictions = self.sess.run([self.prediction], \n",
    "                                    feed_dict = {self.input: self.inputdat, self.label: self.labeldat, \\\n",
    "                                                 self.mode: False})\n",
    "        else:\n",
    "            predictions = self.sess.run([self.prediction], \n",
    "                                    feed_dict = {self.input: self.testdat, self.label: self.testlabeldat, \\\n",
    "                                                 self.mode: False})\n",
    "        predictions = np.array(predictions).reshape((self.num_eg,1))\n",
    "        predictions = predictions.astype(int)\n",
    "        \n",
    "        #Compute error\n",
    "        error = np.sum((predictions - self.labeldat)**2)/self.num_eg\n",
    "        #print(\"error \" + mode, error * 100)\n",
    "        \n",
    "        return predictions, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n2vdata(data, num_eg, n2vdim, vertex):\n",
    "    n2vdata = np.zeros((num_eg, n2vdim, vertex))\n",
    "    for g in range(num_eg):\n",
    "        graph = nx.from_numpy_matrix(data[\"graphs\"][g,:,:])\n",
    "        node2vec = n2v.Node2Vec(graph, dimensions=n2vdim, walk_length=30, num_walks=200, workers=4)\n",
    "        model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "        graphdat = np.zeros((n2vdim, vertex))\n",
    "        for i in range(vertex):\n",
    "            graphdat[:,i] = model.wv.__contains__(str(i))\n",
    "        #print(graphdat)\n",
    "        n2vdata[g,:,:] = graphdat \n",
    "    return n2vdata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 500     |     cost 7.304228e-01     |     error train 38.450     |     error test 39.175\n",
      "epoch 1000     |     cost 6.059466e-01     |     error train 16.200     |     error test 16.950\n",
      "epoch 1500     |     cost 5.481846e-01     |     error train 6.150     |     error test 7.325\n",
      "epoch 2000     |     cost 5.456473e-01     |     error train 5.525     |     error test 6.900\n",
      "epoch 2500     |     cost 5.470282e-01     |     error train 6.025     |     error test 6.875\n",
      "epoch 3000     |     cost 5.465832e-01     |     error train 5.875     |     error test 6.875\n",
      "epoch 3500     |     cost 5.469857e-01     |     error train 5.900     |     error test 6.850\n"
     ]
    }
   ],
   "source": [
    "if  __name__ == \"__main__\":\n",
    "    \n",
    "    num_eg = 4000\n",
    "    vertex = 36\n",
    "    n2vdim = 20\n",
    "    inputdim = vertex * vertex\n",
    "    \n",
    "    #train ensemble \n",
    "    #data = gs.generate_ensemble([int(num_eg/2), int(num_eg/2)], vertex, [1/3, 1/2])\n",
    "    data = gs.generate_ensemble_v3(num_eg, vertex)\n",
    "    \"\"\"n2vdata = np.zeros((num_eg, n2vdim, vertex))\n",
    "    for g in range(num_eg):\n",
    "        graph = nx.from_numpy_matrix(data[\"graphs\"][g,:,:])\n",
    "        node2vec = n2v.Node2Vec(graph, dimensions=n2vdim, walk_length=30, num_walks=200, workers=4)\n",
    "        model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "        graphdat = np.zeros((n2vdim, vertex))\n",
    "        for i in range(vertex):\n",
    "            graphdat[:,i] = model.wv.__contains__(str(i))\n",
    "        #print(graphdat)\n",
    "        n2vdata[g,:,:] = graphdat \n",
    "    data[\"graphs\"] = n2vdata \"\"\"\n",
    "    #test ensemble\n",
    "    #testdata = gs.generate_ensemble([int(num_eg/2), int(num_eg/2)], vertex, [1/3, 1/2])\n",
    "    testdata = gs.generate_ensemble_v3(num_eg, vertex)\n",
    "    \"\"\"n2vtestdata = np.zeros((num_eg, n2vdim, vertex))\n",
    "    for g in range(num_eg):\n",
    "        graph = nx.from_numpy_matrix(testdata[\"graphs\"][g,:,:])\n",
    "        node2vec = n2v.Node2Vec(graph, dimensions=n2vdim, walk_length=6, num_walks=20, workers=4)\n",
    "        model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "        graphdat = np.zeros((n2vdim, vertex))\n",
    "        for i in range(vertex):\n",
    "            graphdat[:,i] = model.wv.__contains__(str(i))\n",
    "        #print(graphdat)\n",
    "        n2vtestdata[g,:,:] = graphdat \n",
    "    testdata[\"graphs\"] = n2vtestdata \"\"\"\n",
    "    \n",
    "    #reformat \n",
    "    data[\"graphs\"] = data[\"graphs\"].reshape((num_eg, -1))\n",
    "    testdata[\"graphs\"] = testdata[\"graphs\"].reshape((num_eg, -1))\n",
    "    \n",
    "    #Fully connected model \n",
    "    FullNN = FullNN(data[\"graphs\"], data[\"labels\"], testdata[\"graphs\"], testdata[\"labels\"], [inputdim,350,300,200,150,100,50,20,1])\n",
    "    \n",
    "    #Test Model \n",
    "    #FullNN.inputdat = testdata[\"graphs\"]\n",
    "    #FullNN.labeldat = testdata[\"labels\"]\n",
    "    #FullNN.trainmode = False\n",
    "    #FullNN.evaluate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
