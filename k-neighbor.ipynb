{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "\n",
      "/home/ubuntu/src/cntk/bindings/python\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python36.zip\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/lib-dynload\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/extensions\n",
      "/home/ubuntu/.ipython\n",
      "Available kernels:\n",
      "  python2           /home/ubuntu/.local/share/jupyter/kernels/python2\n",
      "  pyspark3kernel    /home/ubuntu/anaconda3/envs/tensorflow_p36/share/jupyter/kernels/pyspark3kernel\n",
      "  pysparkkernel     /home/ubuntu/anaconda3/envs/tensorflow_p36/share/jupyter/kernels/pysparkkernel\n",
      "  python3           /home/ubuntu/anaconda3/envs/tensorflow_p36/share/jupyter/kernels/python3\n",
      "  sparkkernel       /home/ubuntu/anaconda3/envs/tensorflow_p36/share/jupyter/kernels/sparkkernel\n",
      "  sparkrkernel      /home/ubuntu/anaconda3/envs/tensorflow_p36/share/jupyter/kernels/sparkrkernel\n",
      "Requirement already satisfied: sklearn in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages\n",
      "Requirement already satisfied: scikit-learn in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from sklearn)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "import sys\n",
    "for p in sys.path:\n",
    "    print(p)\n",
    "!jupyter kernelspec list\n",
    "!pip install sklearn\n",
    "#!pip install networkx\n",
    "#!pip install node2vec\n",
    "import numpy as np\n",
    "import math as math \n",
    "import tensorflow as tf\n",
    "import graph_synthesis as gs\n",
    "from random_mini_batches import random_mini_batches3\n",
    "from random_mini_batches import random_mini_batches2\n",
    "import networkx as nx\n",
    "import node2vec as n2v\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN:\n",
    "    \n",
    "    def __init__(self, inputdat, labeldat, vertexclass, vertexlabel, testdat, testlabeldat, testvertexclass, testvertexlabel, layer_dims, decayrate = 0.99,\n",
    "                 l2reg = 1e-2, learning_rate = 0.008, batch_size = 128, epoch_nums = 3000):\n",
    "        \n",
    "        #Data\n",
    "        self.inputdat = inputdat\n",
    "        self.labeldat = labeldat\n",
    "        self.vertexdat = vertexlabel\n",
    "        self.testdat = testdat\n",
    "        self.testlabeldat = testlabeldat\n",
    "        self.testvertexdat = testvertexlabel\n",
    "        self.num_eg = inputdat.shape[0]\n",
    "        self.inputdim = inputdat.shape[1]\n",
    "        self.vertexdim = vertexlabel.shape[2]\n",
    "        self.vertexclass = vertexclass\n",
    "        self.testvertexclass = testvertexclass\n",
    "        \n",
    "        #Hyperparameters\n",
    "        self.layer_dims = layer_dims\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch_nums = epoch_nums\n",
    "        self.l2reg = l2reg  \n",
    "        self.decayrate = decayrate \n",
    "        \n",
    "        #Initialize\n",
    "        tf.reset_default_graph()\n",
    "        self.initdata()\n",
    "        \n",
    "        #Create graph\n",
    "        self.output, self.gcn = self.model()\n",
    "        self.optimizer, self.cost = self.train()\n",
    "        self.prediction, self.accuracy = self.test()\n",
    "        self.summary_op = self.create_summaries()\n",
    "        \n",
    "        #Run Tensorflow\n",
    "        config = tf.ConfigProto(allow_soft_placement = True, log_device_placement = True, device_count = {'GPU': 1})\n",
    "        self.sess = tf.Session(config = config)\n",
    "        init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        self.sess.run(init)\n",
    "        \n",
    "        #Tensorboard setup\n",
    "        self.writer = tf.summary.FileWriter('./CNNSave',  (self.sess).graph)\n",
    "        \n",
    "        #Train model \n",
    "        self.graph_classifier()\n",
    "        self.prediction_num, self.error_num = self.evaluate(\"train\") \n",
    "        #self.evaluate_cluster()\n",
    "    def  create_summaries(self):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar(\"loss\", self.cost)\n",
    "            #tf.summary.scalar(\"output\", self.output)\n",
    "            tf.summary.scalar(\"accuracy\", self.accuracy)\n",
    "            tf.summary.histogram(\"histogram_loss\", self.cost)\n",
    "            #tf.summary.histogram(\"histogram_loss\", self.output)\n",
    "            summary_op = tf.summary.merge_all()\n",
    "        return summary_op\n",
    "    \n",
    "    def initdata(self):\n",
    "        self.tflearnrate = tf.placeholder(tf.float32, name = \"learnrate\")\n",
    "        self.mode = tf.placeholder(tf.bool, name = \"mode\")\n",
    "        self.input = tf.placeholder(tf.float32, shape = [None, self.inputdim, self.inputdim], name = \"input\")\n",
    "        self.vertexlabel = tf.placeholder(tf.float32, shape = [None, self.inputdim, self.vertexdim], name = \"vertex\")\n",
    "        self.label = tf.placeholder(tf.float32, shape = [None, 1], name = \"label\")\n",
    "\n",
    "    \n",
    "    def model(self):\n",
    "        layer_dims = self.layer_dims\n",
    "        layers = {}\n",
    "        \n",
    "        inputA = self.input\n",
    "        inputX = self.vertexlabel\n",
    "        \n",
    "        scopegcn = 'GCN'\n",
    "        with tf.variable_scope(scopegcn):\n",
    "            #GCN 1 \n",
    "            #name = 'GCN' + str(1)\n",
    "            gcn = self.graph_convolutional_layer(inputX, inputA, scopegcn, 20)\n",
    "        scope = 'batchnorm'+str(0)\n",
    "        with tf.variable_scope(scope):\n",
    "            gcn = tf.contrib.layers.batch_norm(gcn, is_training = self.mode, scope = 'batchnorm')\n",
    "        for i in range(4):\n",
    "            with tf.variable_scope(scopegcn, reuse = True):\n",
    "                gcn = self.graph_convolutional_layer(gcn, inputA, scopegcn, 20)\n",
    "            scope = 'batchnorm'+str(i+1)\n",
    "            with tf.variable_scope(scope):\n",
    "                gcn = tf.contrib.layers.batch_norm(gcn, is_training = self.mode, scope = 'batchnorm')\n",
    "            \n",
    "        \"\"\"#GCN 1 \n",
    "        name = 'GCN' + str(1)\n",
    "        gcn1 = self.graph_convolutional_layer(inputX, inputA, name, 15) \n",
    "        \n",
    "        #GCN 2 \n",
    "        name = 'GCN' + str(2)\n",
    "        gcn2 = self.graph_convolutional_layer(gcn1, inputA, name, 10) \n",
    "        \n",
    "        #GCN 3\n",
    "        name = 'GCN' + str(3)\n",
    "        gcn3 = self.graph_convolutional_layer(gcn2, inputA, name, 5)\"\"\"\n",
    "        \n",
    "        size = gcn.shape[1]\n",
    "        #print(size)\n",
    "        pool_flat = gcn[:,size-1,:]\n",
    "        #pool_flat = tf.reshape(tf.reduce_mean(gcn, axis = 2), [-1, size])\n",
    "    \n",
    "        layers[0] = pool_flat\n",
    "        #Dense layers\n",
    "        for i in range(len(layer_dims)-1):\n",
    "            if (i < len(layer_dims) - 2):\n",
    "                scope = 'denselayers' + str(i+1)\n",
    "                with tf.variable_scope(scope):\n",
    "                    layers[i+1] = tf.contrib.layers.fully_connected(num_outputs = layer_dims[i+1], \\\n",
    "                                                        activation_fn = None, inputs = layers[i], scope = 'dense')\n",
    "                    layers[i+1] = tf.contrib.layers.batch_norm(layers[i+1], is_training = self.mode, scope = 'batchnorm')\n",
    "                    layers[i+1] = tf.nn.relu(layers[i+1], 'relu')\n",
    "            else:\n",
    "                scope = 'denselayers' + str(i+1)\n",
    "                with tf.variable_scope(scope):\n",
    "                    layers[len(layer_dims)-1] = tf.layers.dense(units = layer_dims[len(layer_dims)-1], \\\n",
    "                                                    activation = tf.nn.sigmoid, inputs = layers[len(layer_dims)-2])\n",
    "        \n",
    "        output = layers[len(layer_dims)-1]\n",
    "        return output, gcn  \n",
    "    \n",
    "    def train(self):\n",
    "        #Saver for data \n",
    "        self.saver = tf.train.Saver()\n",
    "        self.global_step = tf.Variable(0, dtype = tf.int32, trainable = False, name = 'global_step')\n",
    "        \n",
    "        #Define cost \n",
    "        gap_loss = tf.reduce_max(self.output) - tf.reduce_min(self.output)\n",
    "        reg_loss = self.l2reg * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()) \n",
    "        cost = (tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = self.output, labels = self.label)) + \\\n",
    "                                                                                            reg_loss)\n",
    "        #cost = (tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = self.output, labels = self.label)) + \\\n",
    "                                                                                            #reg_loss) * 1/(0.01+gap_loss)\n",
    "        \n",
    "        #Batch Normalization\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.tflearnrate).minimize(cost, global_step = self.global_step)\n",
    "            \n",
    "        return optimizer, cost \n",
    "    \n",
    "    def test(self):\n",
    "        prediction = tf.greater(self.output, tf.constant(0.5, dtype = tf.float32))\n",
    "        prediction = tf.cast(prediction, dtype = tf.float32)\n",
    "        with tf.name_scope('accuracy'):\n",
    "            with tf.name_scope('correct_prediction'):\n",
    "                correct_prediction = tf.equal(tf.cast(prediction, dtype = tf.float32), self.label)\n",
    "            with tf.name_scope('accuracy'):\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "                tf.summary.scalar('accuracy', accuracy)\n",
    "        return prediction, accuracy\n",
    "    \n",
    "    def graph_classifier(self):\n",
    "        curlearnrate = self.learning_rate\n",
    "        step = 0 \n",
    "        for epochs in range(self.epoch_nums):\n",
    "            output = []\n",
    "            mini_batch = random_mini_batches3(self.inputdat, self.labeldat, self.vertexdat, self.batch_size)\n",
    "            curlearnrate = curlearnrate * self.decayrate \n",
    "            for dat in mini_batch:\n",
    "                step = step + 1 \n",
    "                _, cost, output, summary = self.sess.run([self.optimizer, self.cost, self.output, self.summary_op], \n",
    "                                                feed_dict = {self.input: dat[0], self.label: dat[1], self.vertexlabel: dat[2],\\\n",
    "                                                             self.mode: True, self.tflearnrate: curlearnrate})\n",
    "                (self.writer).add_summary(summary, global_step = step)\n",
    "            if ((epochs+1) % 50 == 0):\n",
    "                \n",
    "                _, errortrain = self.evaluate(\"train\")\n",
    "                output, cost, accuracy = self.sess.run([self.output, self.cost, self.accuracy], \\\n",
    "                        feed_dict = {self.input: self.inputdat, self.label: self.labeldat, self.vertexlabel: self.vertexdat, \\\n",
    "                                     self.mode: False, self.tflearnrate: curlearnrate})\n",
    "                (self.saver).save(self.sess, 'checkpoints/CNN', global_step=self.global_step)\n",
    "                _, errortest = self.evaluate(\"test\")\n",
    "                print(\"epoch\", epochs + 1, \"    |    \", \"cost\", \"%.6e\" % cost, \"    |    \", \n",
    "                      \"error train\", \"%.3f\" % (errortrain * 100), \"    |    \", \"error test\", \"%.3f\" % (errortest * 100), \n",
    "                      \"   |   \", \"output gap\", \"%.6e\" % (np.max(output) - np.min(output)))\n",
    "            \n",
    "                #if ((epochs+1) % 200 == 0):\n",
    "                #    print(output[0], output[len(output)-1])\n",
    "                \n",
    "    def evaluate(self, mode):\n",
    "        #Get predictions\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            predictions = self.sess.run([self.prediction], \n",
    "                                    feed_dict = {self.input: self.inputdat, self.label: self.labeldat, self.vertexlabel: self.vertexdat,\\\n",
    "                                                 self.mode: False})\n",
    "            predictions = np.array(predictions).reshape((self.num_eg,1))\n",
    "            #predictions = predictions.astype(int)\n",
    "        \n",
    "            #Compute error\n",
    "            error = np.sum((predictions - self.labeldat)**2)/self.num_eg\n",
    "            #print(\"error \" + mode, error * 100)\n",
    "        else:\n",
    "            predictions, gcn = self.sess.run([self.prediction, self.gcn], \n",
    "                                    feed_dict = {self.input: self.testdat, self.label: self.testlabeldat, self.vertexlabel: self.testvertexdat,\\\n",
    "                                                 self.mode: False})\n",
    "            predictions = np.array(predictions).reshape((self.num_eg,1))\n",
    "            #predictions = predictions.astype(int)\n",
    "        \n",
    "            #Compute error\n",
    "            error = np.sum((predictions - self.testlabeldat)**2)/self.num_eg\n",
    "            #print(\"error \" + mode, error * 100)\n",
    "            \n",
    "        return predictions, error\n",
    "    \n",
    "    def evaluate_cluster(self):\n",
    "        #Get predictions\n",
    "        \n",
    "        predictions, gcn = self.sess.run([self.prediction, self.gcn], \n",
    "                                    feed_dict = {self.input: self.testdat, self.label: self.testlabeldat, self.vertexlabel: self.testvertexdat,\\\n",
    "                                                 self.mode: False})\n",
    "        print(gcn[0,0:self.inputdim,:])\n",
    "        \n",
    "        for i in range(10):\n",
    "            graph = gcn[i,0:self.inputdim,:]\n",
    "            label = self.vertexclass[i,:]\n",
    "            kmeans = KMeans(n_clusters=2, random_state=0).fit(graph)\n",
    "            print(label)\n",
    "            print(kmeans)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def graph_convolutional_layer(self, X, An, name, dim):\n",
    "        \n",
    "        AX = tf.matmul(An, X)\n",
    "        btsize = tf.shape(AX)[0]\n",
    "        vertex = AX.shape[2]\n",
    "        \n",
    "        #AX = tf.tile(tf.expand_dims(AX, 1), [1, rep, 1, 1])\n",
    "        \n",
    "        #print(btsize)\n",
    "        #print(vertex)\n",
    "        #with tf.variable_scope(name):\n",
    "        W = tf.get_variable('W', [vertex, dim])\n",
    "        mW = tf.tile(tf.expand_dims(W, 0), [btsize, 1, 1]) \n",
    "        \n",
    "        output = tf.matmul(AX,mW)\n",
    "        \n",
    "        output = tf.nn.relu(output)\n",
    "        #print(output.shape)\n",
    "        #print(output.shape)\n",
    "        return output \n",
    "    \n",
    "    def gated_graph_convolutional_layer(self, X, An, name, dim):\n",
    "        \n",
    "        AX = tf.matmul(An, X)\n",
    "        btsize = tf.shape(AX)[0]\n",
    "        numver = AX.shape[1]\n",
    "        vertex = AX.shape[2]\n",
    "        \n",
    "        #AX = tf.tile(tf.expand_dims(AX, 1), [1, rep, 1, 1])\n",
    "        \n",
    "        #print(btsize)\n",
    "        #print(vertex)\n",
    "        #with tf.variable_scope(name):\n",
    "        W = tf.get_variable('W', [vertex, dim])\n",
    "        mW = tf.tile(tf.expand_dims(W, 0), [btsize, 1, 1]) \n",
    "        \n",
    "        Wn = tf.get_variable('Wn', [vertex, dim])\n",
    "        mWn = tf.tile(tf.expand_dims(Wn, 0), [btsize, 1, 1]) \n",
    "        \n",
    "        #bW = tf.get_variable('bW', [numver, dim])\n",
    "        #mbW = tf.tile(tf.expand_dims(bW, 0), [btsize, 1, 1]) \n",
    "        \n",
    "        output = tf.matmul(AX,mW) + tf.matmul(X,mWn) \n",
    "        \n",
    "        output = tf.nn.relu(output)\n",
    "        \n",
    "        G = tf.get_variable('G', [vertex, dim])\n",
    "        mG = tf.tile(tf.expand_dims(G, 0), [btsize, 1, 1]) \n",
    "        \n",
    "        Gn = tf.get_variable('Gn', [vertex, dim])\n",
    "        mGn = tf.tile(tf.expand_dims(Gn, 0), [btsize, 1, 1]) \n",
    "        \n",
    "        #bG = tf.get_variable('bG', [numver, dim])\n",
    "        #mbG = tf.tile(tf.expand_dims(bG, 0), [btsize, 1, 1]) \n",
    "        \n",
    "        gate = tf.matmul(AX,mG) + tf.matmul(X,mGn)  \n",
    "        \n",
    "        gate = tf.nn.sigmoid(gate)\n",
    "        \n",
    "        output = tf.multiply((1-gate), X) + tf.multiply(gate, output)\n",
    "        #print(output.shape)\n",
    "        #print(output.shape)\n",
    "        return output \n",
    "    \n",
    "    #def graph_local_convolution(self, X):\n",
    "    #    vertex = self.inputdim\n",
    "        \n",
    "    #    for i in range(vertex):\n",
    "            \n",
    "    #    W = tf.get_variable(name + 'W', [vertex, dim])\n",
    "    #    mW = tf.tile(tf.expand_dims(W, 0), [btsize, 1, 1]) \n",
    "    #    return None\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05571031 0.03676471 0.02631579 ... 0.02392344 0.03082192 0.02702703]\n",
      " [0.02785515 0.05514706 0.02255639 ... 0.02392344 0.0239726  0.02702703]\n",
      " [0.01949861 0.02205882 0.05639098 ... 0.0215311  0.0239726  0.02702703]\n",
      " ...\n",
      " [0.02785515 0.03676471 0.03383459 ... 0.05502392 0.03082192 0.02702703]\n",
      " [0.02506964 0.02573529 0.02631579 ... 0.0215311  0.05821918 0.02702703]\n",
      " [0.00278552 0.00367647 0.0037594  ... 0.00239234 0.00342466 0.02702703]]\n",
      "epoch 50     |     cost 7.344568e-01     |     error train 50.000     |     error test 50.000    |    output gap 5.170703e-06\n",
      "epoch 100     |     cost 7.297815e-01     |     error train 50.000     |     error test 50.000    |    output gap 0.000000e+00\n",
      "epoch 150     |     cost 7.275183e-01     |     error train 50.000     |     error test 50.000    |    output gap 2.289861e-04\n",
      "epoch 200     |     cost 7.258125e-01     |     error train 50.000     |     error test 50.000    |    output gap 9.187162e-04\n",
      "epoch 250     |     cost 7.248008e-01     |     error train 50.000     |     error test 50.000    |    output gap 1.648590e-04\n",
      "epoch 300     |     cost 7.246081e-01     |     error train 50.000     |     error test 50.000    |    output gap 7.670186e-04\n",
      "epoch 350     |     cost 7.245452e-01     |     error train 50.000     |     error test 50.000    |    output gap 4.414469e-05\n",
      "epoch 400     |     cost 7.246563e-01     |     error train 50.000     |     error test 50.000    |    output gap 9.022355e-04\n",
      "epoch 450     |     cost 8.159103e-01     |     error train 50.000     |     error test 50.000    |    output gap 0.000000e+00\n",
      "epoch 500     |     cost 8.281322e-01     |     error train 50.000     |     error test 50.000    |    output gap 0.000000e+00\n",
      "epoch 550     |     cost 8.328986e-01     |     error train 50.000     |     error test 50.000    |    output gap 0.000000e+00\n",
      "epoch 600     |     cost 8.346647e-01     |     error train 50.000     |     error test 50.000    |    output gap 0.000000e+00\n",
      "epoch 650     |     cost 8.351687e-01     |     error train 50.000     |     error test 50.000    |    output gap 0.000000e+00\n",
      "epoch 700     |     cost 7.238048e-01     |     error train 49.950     |     error test 50.000    |    output gap 9.464902e-01\n",
      "epoch 750     |     cost 7.246363e-01     |     error train 50.000     |     error test 50.000    |    output gap 9.978004e-05\n",
      "epoch 800     |     cost 7.246375e-01     |     error train 50.000     |     error test 50.000    |    output gap 9.666663e-05\n",
      "epoch 850     |     cost 7.246387e-01     |     error train 50.000     |     error test 50.000    |    output gap 3.126822e-05\n",
      "epoch 900     |     cost 8.356155e-01     |     error train 50.000     |     error test 50.000    |    output gap 0.000000e+00\n",
      "epoch 950     |     cost 8.356140e-01     |     error train 50.000     |     error test 50.000    |    output gap 0.000000e+00\n",
      "epoch 1000     |     cost 8.356055e-01     |     error train 50.000     |     error test 50.000    |    output gap 0.000000e+00\n",
      "epoch 1050     |     cost 8.356127e-01     |     error train 50.000     |     error test 50.000    |    output gap 0.000000e+00\n",
      "epoch 1100     |     cost 8.356187e-01     |     error train 50.000     |     error test 50.000    |    output gap 0.000000e+00\n",
      "epoch 1150     |     cost 8.356203e-01     |     error train 50.000     |     error test 50.000    |    output gap 0.000000e+00\n",
      "epoch 1200     |     cost 6.436478e-01     |     error train 27.750     |     error test 28.250    |    output gap 9.329686e-01\n",
      "epoch 1250     |     cost 7.155468e-01     |     error train 47.750     |     error test 47.700    |    output gap 9.365573e-01\n",
      "epoch 1300     |     cost 7.105013e-01     |     error train 46.450     |     error test 46.400    |    output gap 9.356402e-01\n",
      "epoch 1350     |     cost 6.850713e-01     |     error train 39.200     |     error test 40.100    |    output gap 9.341994e-01\n",
      "epoch 1400     |     cost 6.466065e-01     |     error train 28.650     |     error test 29.300    |    output gap 9.329007e-01\n",
      "epoch 1450     |     cost 6.418475e-01     |     error train 27.350     |     error test 28.150    |    output gap 9.327334e-01\n",
      "epoch 1500     |     cost 6.365916e-01     |     error train 25.750     |     error test 26.150    |    output gap 9.325613e-01\n",
      "epoch 1550     |     cost 6.271259e-01     |     error train 23.450     |     error test 22.750    |    output gap 9.323274e-01\n",
      "epoch 1600     |     cost 6.295651e-01     |     error train 23.850     |     error test 23.500    |    output gap 9.323872e-01\n",
      "epoch 1650     |     cost 6.286252e-01     |     error train 23.800     |     error test 23.100    |    output gap 9.323460e-01\n",
      "epoch 1700     |     cost 6.290017e-01     |     error train 23.800     |     error test 23.400    |    output gap 9.323674e-01\n",
      "epoch 1750     |     cost 6.288119e-01     |     error train 23.800     |     error test 23.250    |    output gap 9.323663e-01\n",
      "epoch 1800     |     cost 6.288993e-01     |     error train 23.800     |     error test 23.300    |    output gap 9.323967e-01\n",
      "epoch 1850     |     cost 6.286998e-01     |     error train 23.800     |     error test 23.200    |    output gap 9.323607e-01\n",
      "epoch 1900     |     cost 6.286049e-01     |     error train 23.800     |     error test 23.100    |    output gap 9.323395e-01\n",
      "epoch 1950     |     cost 6.287031e-01     |     error train 23.800     |     error test 23.200    |    output gap 9.323492e-01\n",
      "epoch 2000     |     cost 6.285863e-01     |     error train 23.800     |     error test 23.100    |    output gap 9.323529e-01\n",
      "epoch 2050     |     cost 6.285015e-01     |     error train 23.800     |     error test 23.100    |    output gap 9.323558e-01\n",
      "epoch 2100     |     cost 6.285103e-01     |     error train 23.800     |     error test 23.100    |    output gap 9.323391e-01\n",
      "epoch 2150     |     cost 6.285658e-01     |     error train 23.800     |     error test 23.100    |    output gap 9.323016e-01\n",
      "epoch 2200     |     cost 6.284419e-01     |     error train 23.800     |     error test 23.100    |    output gap 9.323176e-01\n",
      "epoch 2250     |     cost 6.285356e-01     |     error train 23.800     |     error test 23.100    |    output gap 9.323043e-01\n",
      "epoch 2300     |     cost 6.286992e-01     |     error train 23.800     |     error test 23.200    |    output gap 9.323265e-01\n",
      "epoch 2350     |     cost 6.284795e-01     |     error train 23.800     |     error test 23.100    |    output gap 9.323660e-01\n",
      "epoch 2400     |     cost 6.285185e-01     |     error train 23.800     |     error test 23.100    |    output gap 9.323471e-01\n",
      "epoch 2450     |     cost 6.284189e-01     |     error train 23.750     |     error test 23.100    |    output gap 9.323317e-01\n",
      "epoch 2500     |     cost 6.285158e-01     |     error train 23.800     |     error test 23.100    |    output gap 9.323298e-01\n",
      "epoch 2550     |     cost 6.284113e-01     |     error train 23.750     |     error test 23.100    |    output gap 9.323301e-01\n",
      "epoch 2600     |     cost 6.284087e-01     |     error train 23.750     |     error test 23.100    |    output gap 9.323255e-01\n",
      "epoch 2650     |     cost 6.283336e-01     |     error train 23.750     |     error test 23.050    |    output gap 9.323492e-01\n",
      "epoch 2700     |     cost 6.284698e-01     |     error train 23.800     |     error test 23.100    |    output gap 9.323217e-01\n",
      "epoch 2750     |     cost 6.284544e-01     |     error train 23.800     |     error test 23.100    |    output gap 9.323286e-01\n",
      "epoch 2800     |     cost 6.284501e-01     |     error train 23.800     |     error test 23.100    |    output gap 9.322987e-01\n",
      "epoch 2850     |     cost 6.283677e-01     |     error train 23.750     |     error test 23.050    |    output gap 9.323073e-01\n",
      "epoch 2900     |     cost 6.284342e-01     |     error train 23.800     |     error test 23.100    |    output gap 9.322994e-01\n",
      "epoch 2950     |     cost 6.283944e-01     |     error train 23.750     |     error test 23.100    |    output gap 9.323611e-01\n",
      "epoch 3000     |     cost 6.283476e-01     |     error train 23.750     |     error test 23.100    |    output gap 9.323727e-01\n"
     ]
    }
   ],
   "source": [
    "if  __name__ == \"__main__\":\n",
    "    \n",
    "    k=2\n",
    "    num_eg = 2000\n",
    "    vertex = 37\n",
    "    vertexdim = 20 \n",
    "    inputdim = vertexdim\n",
    "    \n",
    "    #train ensemble \n",
    "    #data = gs.generate_ensemble([int(num_eg/2), int(num_eg/2)], vertex, [1/3, 1/2])\n",
    "    data = gs.generate_ensemble_v2_label(num_eg, vertex-1)\n",
    "    #data = gs.generate_ensemble_same_label(num_eg, vertex-1)\n",
    "    \n",
    "    \n",
    "    #Normalize input:\n",
    "    datagraph = data[\"graphs\"]\n",
    "    for i in range(num_eg):\n",
    "        curgraph = np.linalg.matrix_power(datagraph[i,:,:],k) + np.eye(vertex-1)\n",
    "        #degree = np.sum(curgraph, axis = 0)\n",
    "        #updategraph = np.minimum(curgraph, np.ones((vertex-1,vertex-1)))\n",
    "        data[\"graphs\"][i,:,:] = curgraph \n",
    "    \n",
    "    data = gs.augment(data)\n",
    "    for i in range(num_eg):\n",
    "        curgraph = data[\"graphs\"][i,:,:]\n",
    "        degree = np.sum(curgraph, axis = 0)\n",
    "        updategraph = 1/np.sqrt(degree) * curgraph * 1/np.sqrt(degree)\n",
    "        data[\"graphs\"][i,:,:] = updategraph\n",
    "    datavertex = np.random.normal(loc=1, scale=0.0, size=(num_eg, vertex, vertexdim)).astype(np.float32)\n",
    "    print(data[\"graphs\"][1,:,:])\n",
    "    #data = gs.generate_ensemble_n2v(num_eg, vertex)\n",
    "    #test ensemble\n",
    "    testdata = gs.generate_ensemble_v2_label(num_eg, vertex-1)\n",
    "    #testdata = gs.generate_ensemble([int(num_eg/2), int(num_eg/2)], vertex, [1/3, 1/2])\n",
    "    #testdata = gs.generate_ensemble_same_label(num_eg, vertex-1)\n",
    "    \n",
    "    \n",
    "    testdatagraph = testdata[\"graphs\"]\n",
    "    for i in range(num_eg):\n",
    "        curgraph = np.linalg.matrix_power(testdatagraph[i,:,:],2) + np.eye(vertex-1)\n",
    "        #degree = np.sum(curgraph, axis = 0)\n",
    "        #updategraph = np.minimum(curgraph, np.ones((vertex-1,vertex-1)))\n",
    "        testdata[\"graphs\"][i,:,:] = curgraph \n",
    "    testdata = gs.augment(testdata)\n",
    "    for i in range(num_eg):\n",
    "        curgraph = testdata[\"graphs\"][i,:,:]\n",
    "        degree = np.sum(curgraph, axis = 0)\n",
    "        updategraph = 1/np.sqrt(degree) * curgraph * 1/np.sqrt(degree)\n",
    "        testdata[\"graphs\"][i,:,:] = updategraph \n",
    "        \n",
    "    testdatavertex = np.random.normal(loc=1, scale=0.0, size=(num_eg, vertex, vertexdim)).astype(np.float32)\n",
    "    #testdata = gs.generate_ensemble_n2v(num_eg, vertex)\n",
    "    #reformat \n",
    "    data[\"graphs\"] = data[\"graphs\"].reshape((num_eg, vertex, vertex))\n",
    "    testdata[\"graphs\"] = testdata[\"graphs\"].reshape((num_eg, vertex, vertex))\n",
    "    \n",
    "    #Fully connected model \n",
    "    GCN = GCN(data[\"graphs\"], data[\"labels\"], data[\"vertex\"], datavertex, testdata[\"graphs\"], testdata[\"labels\"], testdata[\"vertex\"], testdatavertex, [inputdim,15,10,5,1])\n",
    "    \n",
    "    #Test Model \n",
    "    #FullNN.inputdat = testdata[\"graphs\"]\n",
    "    #FullNN.labeldat = testdata[\"labels\"]\n",
    "    #FullNN.trainmode = False\n",
    "    #FullNN.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4444444444444444\n",
      "0.5\n",
      "0.41666666666666663\n",
      "0.33333333333333337\n",
      "0.4444444444444444\n",
      "0.4444444444444445\n",
      "0.4166666666666667\n",
      "0.5\n",
      "0.3888888888888889\n",
      "0.4444444444444445\n",
      "0.4444444444444444\n",
      "0.4722222222222222\n",
      "0.38888888888888884\n",
      "0.4444444444444444\n",
      "0.5\n",
      "0.41666666666666663\n",
      "0.4444444444444444\n",
      "0.3888888888888889\n",
      "0.3611111111111111\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.5\n",
      "0.4722222222222222\n",
      "0.4444444444444445\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.4444444444444444\n",
      "0.3611111111111111\n",
      "0.4444444444444444\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.4166666666666667\n",
      "0.4444444444444444\n",
      "0.4444444444444445\n",
      "0.4166666666666667\n",
      "0.4444444444444445\n",
      "0.5\n",
      "0.3333333333333333\n",
      "0.4444444444444445\n",
      "0.4166666666666667\n",
      "0.3611111111111111\n",
      "0.5\n",
      "0.41666666666666663\n",
      "0.5\n",
      "0.5\n",
      "0.41666666666666663\n",
      "0.4722222222222222\n",
      "0.38888888888888884\n",
      "0.5\n",
      "0.4722222222222222\n",
      "0.4444444444444444\n",
      "0.4722222222222222\n",
      "0.4444444444444445\n",
      "0.4444444444444444\n",
      "0.4444444444444444\n",
      "0.41666666666666663\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.41666666666666663\n",
      "0.41666666666666663\n",
      "0.3611111111111111\n",
      "0.4444444444444444\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.41666666666666663\n",
      "0.5\n",
      "0.4444444444444444\n",
      "0.41666666666666663\n",
      "0.3888888888888889\n",
      "0.5\n",
      "0.25\n",
      "0.4722222222222222\n",
      "0.41666666666666663\n",
      "0.41666666666666663\n",
      "0.33333333333333337\n",
      "0.4444444444444445\n",
      "0.41666666666666663\n",
      "0.5\n",
      "0.4444444444444444\n",
      "0.38888888888888884\n",
      "0.3333333333333333\n",
      "0.4444444444444444\n",
      "0.5\n",
      "0.4722222222222222\n",
      "0.5\n",
      "0.4166666666666667\n",
      "0.4444444444444444\n",
      "0.4722222222222222\n",
      "0.41666666666666663\n",
      "0.33333333333333337\n",
      "0.5\n",
      "0.4722222222222222\n",
      "0.4444444444444444\n",
      "0.4444444444444444\n",
      "0.3888888888888889\n",
      "0.5\n",
      "0.4722222222222222\n",
      "0.5\n",
      "0.4166666666666667\n",
      "0.5\n",
      "0.4444444444444444\n",
      "0.4444444444444444\n",
      "0.5\n",
      "0.3333333333333333\n",
      "0.3888888888888889\n",
      "0.3888888888888889\n",
      "0.3888888888888889\n",
      "0.4722222222222222\n",
      "0.4166666666666667\n",
      "0.4722222222222222\n",
      "0.5\n",
      "0.4444444444444444\n",
      "0.3611111111111111\n",
      "0.4722222222222222\n",
      "0.38888888888888884\n",
      "0.5\n",
      "0.41666666666666663\n",
      "0.5\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.3888888888888889\n",
      "0.4166666666666667\n",
      "0.4722222222222222\n",
      "0.38888888888888884\n",
      "0.4166666666666667\n",
      "0.4444444444444445\n",
      "0.4444444444444444\n",
      "0.5\n",
      "0.4166666666666667\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.4444444444444444\n",
      "0.5\n",
      "0.5\n",
      "0.33333333333333337\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.5\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.4166666666666667\n",
      "0.3888888888888889\n",
      "0.4722222222222222\n",
      "0.3611111111111111\n",
      "0.4722222222222222\n",
      "0.4444444444444444\n",
      "0.4444444444444444\n",
      "0.4444444444444444\n",
      "0.4444444444444444\n",
      "0.3888888888888889\n",
      "0.5\n",
      "0.3611111111111111\n",
      "0.3888888888888889\n",
      "0.33333333333333337\n",
      "0.4444444444444445\n",
      "0.4722222222222222\n",
      "0.4444444444444444\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.4166666666666667\n",
      "0.33333333333333337\n",
      "0.5\n",
      "0.3888888888888889\n",
      "0.5\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.33333333333333337\n",
      "0.4166666666666667\n",
      "0.41666666666666663\n",
      "0.4722222222222222\n",
      "0.5\n",
      "0.41666666666666663\n",
      "0.4722222222222222\n",
      "0.5\n",
      "0.5\n",
      "0.4722222222222222\n",
      "0.4444444444444444\n",
      "0.3055555555555556\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.5\n",
      "0.4166666666666667\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.4166666666666667\n",
      "0.33333333333333337\n",
      "0.4166666666666667\n",
      "0.3611111111111111\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.41666666666666663\n",
      "0.4722222222222222\n",
      "0.3888888888888889\n",
      "0.4444444444444444\n",
      "0.4444444444444444\n",
      "0.3888888888888889\n",
      "0.41666666666666663\n",
      "0.4444444444444444\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.4444444444444444\n",
      "0.4444444444444444\n",
      "0.4722222222222222\n",
      "0.5\n",
      "0.4444444444444445\n",
      "0.4444444444444444\n",
      "0.3888888888888889\n",
      "0.4722222222222222\n",
      "0.41666666666666663\n",
      "0.38888888888888884\n",
      "0.4444444444444444\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.3888888888888889\n",
      "0.41666666666666663\n",
      "0.5\n",
      "0.3055555555555556\n",
      "0.41666666666666663\n",
      "0.4444444444444444\n",
      "0.4722222222222222\n",
      "0.4166666666666667\n",
      "0.4166666666666667\n",
      "0.4722222222222222\n",
      "0.5\n",
      "0.3611111111111111\n",
      "0.38888888888888884\n",
      "0.41666666666666663\n",
      "0.4444444444444444\n",
      "0.3611111111111111\n",
      "0.4166666666666667\n",
      "0.4444444444444444\n",
      "0.41666666666666663\n",
      "0.4722222222222222\n",
      "0.2777777777777778\n",
      "0.4722222222222222\n",
      "0.3888888888888889\n",
      "0.4444444444444444\n",
      "0.4166666666666667\n",
      "0.3611111111111111\n",
      "0.3611111111111111\n",
      "0.5\n",
      "0.4722222222222222\n",
      "0.3888888888888889\n",
      "0.3888888888888889\n",
      "0.4722222222222222\n",
      "0.4444444444444444\n",
      "0.4722222222222222\n",
      "0.4444444444444444\n",
      "0.3055555555555556\n",
      "0.4444444444444444\n",
      "0.3888888888888889\n",
      "0.4722222222222222\n",
      "0.4444444444444444\n",
      "0.4722222222222222\n",
      "0.36111111111111116\n",
      "0.3611111111111111\n",
      "0.41666666666666663\n",
      "0.4444444444444444\n",
      "0.4444444444444444\n",
      "0.5\n",
      "0.4722222222222222\n",
      "0.4444444444444444\n",
      "0.4722222222222222\n",
      "0.4444444444444444\n",
      "0.3888888888888889\n",
      "0.33333333333333337\n",
      "0.4444444444444444\n",
      "0.41666666666666663\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.4444444444444444\n",
      "0.5\n",
      "0.4444444444444444\n",
      "0.4444444444444444\n",
      "0.33333333333333337\n",
      "0.2777777777777778\n",
      "0.4444444444444444\n",
      "0.3888888888888889\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.5\n",
      "0.4722222222222222\n",
      "0.4444444444444444\n",
      "0.41666666666666663\n",
      "0.4722222222222222\n",
      "0.5\n",
      "0.3888888888888889\n",
      "0.4444444444444444\n",
      "0.5\n",
      "0.4722222222222222\n",
      "0.41666666666666663\n",
      "0.41666666666666663\n",
      "0.41666666666666663\n",
      "0.4444444444444445\n",
      "0.4166666666666667\n",
      "0.5\n",
      "0.4722222222222222\n",
      "0.3888888888888889\n",
      "0.3611111111111111\n",
      "0.33333333333333337\n",
      "0.4444444444444444\n",
      "0.4444444444444444\n",
      "0.4444444444444444\n",
      "0.4444444444444444\n",
      "0.4722222222222222\n",
      "0.3888888888888889\n",
      "0.5\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.41666666666666663\n",
      "0.5\n",
      "0.4444444444444444\n",
      "0.41666666666666663\n",
      "0.4722222222222222\n",
      "0.4444444444444444\n",
      "0.38888888888888884\n",
      "0.4444444444444444\n",
      "0.5\n",
      "0.4444444444444444\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.5\n",
      "0.4444444444444444\n",
      "0.3333333333333333\n",
      "0.5\n",
      "0.3611111111111111\n",
      "0.4166666666666667\n",
      "0.4444444444444444\n",
      "0.38888888888888884\n",
      "0.3333333333333333\n",
      "0.3888888888888889\n",
      "0.4166666666666667\n",
      "0.4722222222222222\n",
      "0.38888888888888884\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.41666666666666663\n",
      "0.4166666666666667\n",
      "0.41666666666666663\n",
      "0.4722222222222222\n",
      "0.38888888888888884\n",
      "0.5\n",
      "0.4722222222222222\n",
      "0.3888888888888889\n",
      "0.4444444444444444\n",
      "0.5\n",
      "0.5\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.41666666666666663\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.5\n",
      "0.4722222222222222\n",
      "0.3055555555555556\n",
      "0.41666666666666663\n",
      "0.4444444444444444\n",
      "0.4722222222222222\n",
      "0.41666666666666663\n",
      "0.5\n",
      "0.3611111111111111\n",
      "0.41666666666666663\n",
      "0.3611111111111111\n",
      "0.4444444444444444\n",
      "0.4444444444444444\n",
      "0.4444444444444444\n",
      "0.4444444444444444\n",
      "0.4444444444444444\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.5\n",
      "0.4722222222222222\n",
      "0.4166666666666667\n",
      "0.5\n",
      "0.5\n",
      "0.4166666666666667\n",
      "0.5\n",
      "0.3888888888888889\n",
      "0.4166666666666667\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.4166666666666667\n",
      "0.3888888888888889\n",
      "0.3888888888888889\n",
      "0.4166666666666667\n",
      "0.38888888888888884\n",
      "0.4444444444444444\n",
      "0.4444444444444445\n",
      "0.3611111111111111\n",
      "0.4166666666666667\n",
      "0.4722222222222222\n",
      "0.5\n",
      "0.3611111111111111\n",
      "0.4444444444444444\n",
      "0.4444444444444444\n",
      "0.4722222222222222\n",
      "0.5\n",
      "0.41666666666666663\n",
      "0.5\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.4444444444444444\n",
      "0.4722222222222222\n",
      "0.2777777777777778\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.41666666666666663\n",
      "0.3888888888888889\n",
      "0.4444444444444444\n",
      "0.3611111111111111\n",
      "0.3611111111111111\n",
      "0.4444444444444444\n",
      "0.41666666666666663\n",
      "0.4444444444444444\n",
      "0.4444444444444444\n",
      "0.4722222222222222\n",
      "0.4444444444444444\n",
      "0.4722222222222222\n",
      "0.3888888888888889\n",
      "0.4444444444444444\n",
      "0.4444444444444445\n",
      "0.5\n",
      "0.41666666666666663\n",
      "0.3611111111111111\n",
      "0.33333333333333337\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.4444444444444444\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.3333333333333333\n",
      "0.3888888888888889\n",
      "0.41666666666666663\n",
      "0.4444444444444444\n",
      "0.41666666666666663\n",
      "0.41666666666666663\n",
      "0.5\n",
      "0.41666666666666663\n",
      "0.3333333333333333\n",
      "0.4166666666666667\n",
      "0.41666666666666663\n",
      "0.4722222222222222\n",
      "0.41666666666666663\n",
      "0.4722222222222222\n",
      "0.3055555555555555\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.5\n",
      "0.4444444444444444\n",
      "0.4444444444444444\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.4444444444444445\n",
      "0.5\n",
      "0.2777777777777778\n",
      "0.41666666666666663\n",
      "0.4722222222222222\n",
      "0.4444444444444445\n",
      "0.4444444444444444\n",
      "0.41666666666666663\n",
      "0.4444444444444444\n",
      "0.4444444444444444\n",
      "0.4722222222222222\n",
      "0.5\n",
      "0.4722222222222222\n",
      "0.41666666666666663\n",
      "0.41666666666666663\n",
      "0.41666666666666663\n",
      "0.5\n",
      "0.41666666666666663\n",
      "0.4166666666666667\n",
      "0.4722222222222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4444444444444444\n",
      "0.5\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.5\n",
      "0.5\n",
      "0.3611111111111111\n",
      "0.4444444444444445\n",
      "0.4444444444444444\n",
      "0.4722222222222222\n",
      "0.4722222222222222\n",
      "0.5\n",
      "0.41666666666666663\n",
      "0.4444444444444444\n",
      "0.4166666666666667\n",
      "0.41666666666666663\n",
      "0.4385000000000014\n"
     ]
    }
   ],
   "source": [
    "predictions, gcn = GCN.sess.run([GCN.prediction, GCN.gcn], \n",
    "                                    feed_dict = {GCN.input: GCN.testdat, GCN.label: GCN.testlabeldat, GCN.vertexlabel: GCN.testvertexdat,\\\n",
    "                                                 GCN.mode: False})\n",
    "#print(predictions)\n",
    "#print(gcn[0,0:GCN.inputdim,:])\n",
    "cumerror = 0 \n",
    "for i in range(500):\n",
    "    graph = gcn[i,0:(GCN.inputdim-1),:]\n",
    "    label = GCN.vertexclass[i,:]\n",
    "    kmeans = KMeans(n_clusters=2).fit_predict(graph)\n",
    "    label = label.reshape(int(label.shape[0]))\n",
    "    #print(kmeans)\n",
    "    idx0 = (kmeans == 0)\n",
    "    idx1 = (kmeans == 1)\n",
    "    label0 = label[idx0]\n",
    "    label1 = label[idx1]\n",
    "    error = 0 \n",
    "    if (np.sum(label0) < len(label0)/2):\n",
    "        error =  np.sum(label0)/len(label) + (len(label1) - np.sum(label1))/len(label)\n",
    "    else:\n",
    "        error = (len(label0) - np.sum(label0))/len(label) + np.sum(label1)/len(label)\n",
    "    print(error)\n",
    "    cumerror += error\n",
    "print(cumerror / 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullNN:\n",
    "    \n",
    "    def __init__(self, inputdat, labeldat, testdat, testlabeldat, layer_dims, decayrate = 0.99,\n",
    "                 l2reg = 1e-2, learning_rate = 0.008, batch_size = 128, epoch_nums = 15000):\n",
    "        \n",
    "        #Data\n",
    "        self.inputdat = inputdat\n",
    "        self.labeldat = labeldat\n",
    "        self.testdat = testdat\n",
    "        self.testlabeldat = testlabeldat\n",
    "        self.num_eg = inputdat.shape[0]\n",
    "        \n",
    "        #Hyperparameters\n",
    "        self.layer_dims = layer_dims\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch_nums = epoch_nums\n",
    "        self.l2reg = l2reg  \n",
    "        self.decayrate = decayrate \n",
    "        \n",
    "        #Initialize\n",
    "        tf.reset_default_graph()\n",
    "        self.initdata()\n",
    "        \n",
    "        #Create graph\n",
    "        self.output = self.model()\n",
    "        self.optimizer, self.cost = self.train()\n",
    "        self.prediction, self.accuracy = self.test()\n",
    "        self.summary_op = self.create_summaries()\n",
    "        \n",
    "        #Run Tensorflow\n",
    "        config = tf.ConfigProto(allow_soft_placement = True, log_device_placement = True, device_count = {'GPU': 1})\n",
    "        self.sess = tf.Session(config = config)\n",
    "        init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        self.sess.run(init)\n",
    "        \n",
    "        #Tensorboard setup\n",
    "        self.writer = tf.summary.FileWriter('./FullNNSave',  (self.sess).graph)\n",
    "        \n",
    "        #Train model \n",
    "        self.graph_classifier()\n",
    "        self.prediction_num, self.error_num = self.evaluate(\"train\") \n",
    "        \n",
    "    def  create_summaries(self):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar(\"loss\", self.cost)\n",
    "            tf.summary.scalar(\"accuracy\", self.accuracy)\n",
    "            tf.summary.histogram(\"histogram_loss\", self.cost)\n",
    "            summary_op = tf.summary.merge_all()\n",
    "        return summary_op\n",
    "    \n",
    "    def initdata(self):\n",
    "        self.tflearnrate = tf.placeholder(tf.float32, name = \"learnrate\")\n",
    "        self.mode = tf.placeholder(tf.bool, name = \"mode\")\n",
    "        self.input = tf.placeholder(tf.float32, shape = [None, self.layer_dims[0]], name = \"input\")\n",
    "        self.label = tf.placeholder(tf.float32, shape = [None, 1], name = \"label\")\n",
    "\n",
    "    \n",
    "    def model(self):\n",
    "        layer_dims = self.layer_dims\n",
    "        layers = {}\n",
    "        layers[0] = self.input\n",
    "        for i in range(len(layer_dims)-1):\n",
    "            if (i < len(layer_dims) - 2):\n",
    "                scope = 'layers' + str(i+1)\n",
    "                with tf.variable_scope(scope):\n",
    "                    #layers[i+1] = tf.layers.dense(units = layer_dims[i+1], activation = tf.nn.relu, inputs = layers[i])\n",
    "                    layers[i+1] = tf.contrib.layers.fully_connected(num_outputs = layer_dims[i+1], \\\n",
    "                                                        activation_fn = None, inputs = layers[i], scope = 'dense')\n",
    "                    layers[i+1] = tf.contrib.layers.batch_norm(layers[i+1], is_training = self.mode, scope = 'batchnorm')\n",
    "                    layers[i+1] = tf.nn.relu(layers[i+1], 'relu')\n",
    "            else:\n",
    "                scope = 'layers' + str(i+1)\n",
    "                with tf.variable_scope(scope):\n",
    "                    layers[len(layer_dims)-1] = tf.layers.dense(units = layer_dims[len(layer_dims)-1], \\\n",
    "                                                    activation = tf.nn.sigmoid, inputs = layers[len(layer_dims)-2])\n",
    "        \n",
    "        output = layers[len(layer_dims)-1]\n",
    "        return output \n",
    "    \n",
    "    def train(self):\n",
    "        #Saver for data \n",
    "        self.saver = tf.train.Saver()\n",
    "        self.global_step = tf.Variable(0, dtype = tf.int32, trainable = False, name = 'global_step')\n",
    "        \n",
    "        #Define cost \n",
    "        reg_loss = self.l2reg * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()) \n",
    "        cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = self.output, labels = self.label)) + \\\n",
    "                                                                                                            reg_loss \n",
    "        \n",
    "        #Batch Normalization\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.tflearnrate).minimize(cost, global_step = self.global_step)\n",
    "            \n",
    "        return optimizer, cost \n",
    "    \n",
    "    def test(self):\n",
    "        prediction = tf.greater(self.output, tf.constant(0.5, dtype = tf.float32))\n",
    "        with tf.name_scope('accuracy'):\n",
    "            with tf.name_scope('correct_prediction'):\n",
    "                correct_prediction = tf.equal(tf.cast(prediction, dtype = tf.float32), self.label)\n",
    "            with tf.name_scope('accuracy'):\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "                tf.summary.scalar('accuracy', accuracy)\n",
    "        return prediction, accuracy\n",
    "    \n",
    "    def graph_classifier(self):\n",
    "        curlearnrate = self.learning_rate\n",
    "        step = 0 \n",
    "        for epochs in range(self.epoch_nums):\n",
    "            output = []\n",
    "            mini_batch = random_mini_batches2(self.inputdat, self.labeldat, self.batch_size)\n",
    "            curlearnrate = curlearnrate * self.decayrate \n",
    "            for dat in mini_batch:\n",
    "                step = step + 1 \n",
    "                _, cost, output, summary = self.sess.run([self.optimizer, self.cost, self.output, self.summary_op], \n",
    "                                                feed_dict = {self.input: dat[0], self.label: dat[1], \\\n",
    "                                                             self.mode: True, self.tflearnrate: curlearnrate})\n",
    "                (self.writer).add_summary(summary, global_step = step)\n",
    "            if (epochs % 100 == 0):\n",
    "                _, errortrain = self.evaluate(\"train\")\n",
    "                cost, accuracy = self.sess.run([self.cost, self.accuracy], \\\n",
    "                        feed_dict = {self.input: self.inputdat, self.label: self.labeldat, \\\n",
    "                                     self.mode: False, self.tflearnrate: curlearnrate})\n",
    "                (self.saver).save(self.sess, 'checkpoints/CNN', global_step=self.global_step)\n",
    "                _, errortest = self.evaluate(\"test\")\n",
    "                print(\"epoch\", epochs, \"    |    \", \"cost\", \"%.6e\" % cost, \"    |    \", \n",
    "                      \"error train\", \"%.3f\" % (errortrain * 100), \"    |    \", \"error test\", \"%.3f\" % (errortest * 100))\n",
    "        \n",
    "    def evaluate(self, mode):\n",
    "        #Get predictions\n",
    "        if mode == \"train\":\n",
    "            predictions = self.sess.run([self.prediction], \n",
    "                                    feed_dict = {self.input: self.inputdat, self.label: self.labeldat, \\\n",
    "                                                 self.mode: False})\n",
    "        else:\n",
    "            predictions = self.sess.run([self.prediction], \n",
    "                                    feed_dict = {self.input: self.testdat, self.label: self.testlabeldat, \\\n",
    "                                                 self.mode: False})\n",
    "        predictions = np.array(predictions).reshape((self.num_eg,1))\n",
    "        predictions = predictions.astype(int)\n",
    "        \n",
    "        #Compute error\n",
    "        error = np.sum((predictions - self.labeldat)**2)/self.num_eg\n",
    "        #print(\"error \" + mode, error * 100)\n",
    "        \n",
    "        return predictions, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05639098 0.02364066 0.01111111 ... 0.02453988 0.01644737 0.02702703]\n",
      " [0.03759398 0.05673759 0.03333333 ... 0.04294479 0.02631579 0.02702703]\n",
      " [0.0075188  0.0141844  0.05555556 ... 0.01840491 0.01644737 0.02702703]\n",
      " ...\n",
      " [0.03007519 0.03309693 0.03333333 ... 0.05521472 0.02302632 0.02702703]\n",
      " [0.01879699 0.01891253 0.02777778 ... 0.02147239 0.05263158 0.02702703]\n",
      " [0.0037594  0.00236407 0.00555556 ... 0.00306748 0.00328947 0.02702703]]\n",
      "[[-6.94824159e-02  4.51461971e-03  3.49673256e-03  2.12956965e-02\n",
      "   9.27078128e-02 -1.39359817e-01  1.26771256e-02 -8.46166586e-07\n",
      "  -5.39693795e-03  2.64952779e-02 -1.78540036e-01 -4.33678553e-02\n",
      "   9.68384445e-02  2.96154022e-02 -5.10152727e-02 -3.21727101e-04\n",
      "   9.60365832e-02  1.53666735e-02 -1.39003005e-02 -4.01819088e-02]]\n",
      "epoch 0     |     cost 7.934240e-01     |     error train 50.000     |     error test 50.000\n",
      "epoch 100     |     cost 7.095647e-01     |     error train 50.000     |     error test 50.000\n",
      "epoch 200     |     cost 5.820081e-01     |     error train 7.150     |     error test 6.775\n",
      "epoch 300     |     cost 5.675656e-01     |     error train 5.275     |     error test 5.125\n",
      "epoch 400     |     cost 5.671183e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 500     |     cost 5.671174e-01     |     error train 5.275     |     error test 5.050\n",
      "epoch 600     |     cost 5.670093e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 700     |     cost 5.670564e-01     |     error train 5.250     |     error test 5.175\n",
      "epoch 800     |     cost 5.670391e-01     |     error train 5.250     |     error test 5.175\n",
      "epoch 900     |     cost 5.670395e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 1000     |     cost 5.670400e-01     |     error train 5.250     |     error test 5.125\n",
      "epoch 1100     |     cost 5.670372e-01     |     error train 5.250     |     error test 5.175\n",
      "epoch 1200     |     cost 5.670383e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 1300     |     cost 5.670328e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 1400     |     cost 5.670397e-01     |     error train 5.250     |     error test 5.100\n",
      "epoch 1500     |     cost 5.670347e-01     |     error train 5.250     |     error test 5.125\n",
      "epoch 1600     |     cost 5.670389e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 1700     |     cost 5.670364e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 1800     |     cost 5.670359e-01     |     error train 5.250     |     error test 5.175\n",
      "epoch 1900     |     cost 5.670380e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 2000     |     cost 5.670419e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 2100     |     cost 5.670308e-01     |     error train 5.250     |     error test 5.175\n",
      "epoch 2200     |     cost 5.670382e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 2300     |     cost 5.670316e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 2400     |     cost 5.670341e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 2500     |     cost 5.670345e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 2600     |     cost 5.670371e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 2700     |     cost 5.670394e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 2800     |     cost 5.670418e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 2900     |     cost 5.670314e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 3000     |     cost 5.670446e-01     |     error train 5.250     |     error test 5.100\n",
      "epoch 3100     |     cost 5.670341e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 3200     |     cost 5.670361e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 3300     |     cost 5.670350e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 3400     |     cost 5.670339e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 3500     |     cost 5.670326e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 3600     |     cost 5.670367e-01     |     error train 5.250     |     error test 5.125\n",
      "epoch 3700     |     cost 5.670367e-01     |     error train 5.250     |     error test 5.175\n",
      "epoch 3800     |     cost 5.670370e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 3900     |     cost 5.670349e-01     |     error train 5.250     |     error test 5.125\n",
      "epoch 4000     |     cost 5.670350e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 4100     |     cost 5.670357e-01     |     error train 5.250     |     error test 5.175\n",
      "epoch 4200     |     cost 5.670390e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 4300     |     cost 5.670469e-01     |     error train 5.250     |     error test 5.100\n",
      "epoch 4400     |     cost 5.670410e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 4500     |     cost 5.670435e-01     |     error train 5.250     |     error test 5.175\n",
      "epoch 4600     |     cost 5.670388e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 4700     |     cost 5.670418e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 4800     |     cost 5.670359e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 4900     |     cost 5.670413e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 5000     |     cost 5.670393e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 5100     |     cost 5.670393e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 5200     |     cost 5.670390e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 5300     |     cost 5.670277e-01     |     error train 5.250     |     error test 5.175\n",
      "epoch 5400     |     cost 5.670331e-01     |     error train 5.250     |     error test 5.175\n",
      "epoch 5500     |     cost 5.670345e-01     |     error train 5.250     |     error test 5.125\n",
      "epoch 5600     |     cost 5.670375e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 5700     |     cost 5.670363e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 5800     |     cost 5.670368e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 5900     |     cost 5.670480e-01     |     error train 5.250     |     error test 5.125\n",
      "epoch 6000     |     cost 5.670325e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 6100     |     cost 5.670454e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 6200     |     cost 5.670357e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 6300     |     cost 5.670398e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 6400     |     cost 5.670286e-01     |     error train 5.250     |     error test 5.175\n",
      "epoch 6500     |     cost 5.670447e-01     |     error train 5.250     |     error test 5.125\n",
      "epoch 6600     |     cost 5.670386e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 6700     |     cost 5.670387e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 6800     |     cost 5.670413e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 6900     |     cost 5.670361e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 7000     |     cost 5.670460e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 7100     |     cost 5.670331e-01     |     error train 5.250     |     error test 5.175\n",
      "epoch 7200     |     cost 5.670482e-01     |     error train 5.250     |     error test 5.100\n",
      "epoch 7300     |     cost 5.670307e-01     |     error train 5.250     |     error test 5.175\n",
      "epoch 7400     |     cost 5.670388e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 7500     |     cost 5.670441e-01     |     error train 5.250     |     error test 5.100\n",
      "epoch 7600     |     cost 5.670363e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 7700     |     cost 5.670391e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 7800     |     cost 5.670443e-01     |     error train 5.250     |     error test 5.100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7900     |     cost 5.670444e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 8000     |     cost 5.670366e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 8100     |     cost 5.670445e-01     |     error train 5.250     |     error test 5.100\n",
      "epoch 8200     |     cost 5.670369e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 8300     |     cost 5.670371e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 8400     |     cost 5.670413e-01     |     error train 5.250     |     error test 5.175\n",
      "epoch 8500     |     cost 5.670332e-01     |     error train 5.250     |     error test 5.175\n",
      "epoch 8600     |     cost 5.670383e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 8700     |     cost 5.670311e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 8800     |     cost 5.670320e-01     |     error train 5.250     |     error test 5.175\n",
      "epoch 8900     |     cost 5.670422e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 9000     |     cost 5.670384e-01     |     error train 5.250     |     error test 5.100\n",
      "epoch 9100     |     cost 5.670326e-01     |     error train 5.250     |     error test 5.100\n",
      "epoch 9200     |     cost 5.670344e-01     |     error train 5.250     |     error test 5.175\n",
      "epoch 9300     |     cost 5.670413e-01     |     error train 5.250     |     error test 5.125\n",
      "epoch 9400     |     cost 5.670385e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 9500     |     cost 5.670421e-01     |     error train 5.250     |     error test 5.100\n",
      "epoch 9600     |     cost 5.670369e-01     |     error train 5.250     |     error test 5.100\n",
      "epoch 9700     |     cost 5.670378e-01     |     error train 5.250     |     error test 5.125\n",
      "epoch 9800     |     cost 5.670368e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 9900     |     cost 5.670365e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 10000     |     cost 5.670391e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 10100     |     cost 5.670357e-01     |     error train 5.250     |     error test 5.175\n",
      "epoch 10200     |     cost 5.670443e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 10300     |     cost 5.670391e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 10400     |     cost 5.670311e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 10500     |     cost 5.670345e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 10600     |     cost 5.670464e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 10700     |     cost 5.670429e-01     |     error train 5.250     |     error test 5.125\n",
      "epoch 10800     |     cost 5.670403e-01     |     error train 5.250     |     error test 5.100\n",
      "epoch 10900     |     cost 5.670294e-01     |     error train 5.250     |     error test 5.175\n",
      "epoch 11000     |     cost 5.670355e-01     |     error train 5.250     |     error test 5.100\n",
      "epoch 11100     |     cost 5.670338e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 11200     |     cost 5.670317e-01     |     error train 5.250     |     error test 5.175\n",
      "epoch 11300     |     cost 5.670403e-01     |     error train 5.250     |     error test 5.175\n",
      "epoch 11400     |     cost 5.670334e-01     |     error train 5.250     |     error test 5.175\n",
      "epoch 11500     |     cost 5.670392e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 11600     |     cost 5.670344e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 11700     |     cost 5.670354e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 11800     |     cost 5.670395e-01     |     error train 5.250     |     error test 5.175\n",
      "epoch 11900     |     cost 5.670443e-01     |     error train 5.250     |     error test 5.100\n",
      "epoch 12000     |     cost 5.670346e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 12100     |     cost 5.670395e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 12200     |     cost 5.670398e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 12300     |     cost 5.670372e-01     |     error train 5.275     |     error test 5.175\n",
      "epoch 12400     |     cost 5.670473e-01     |     error train 5.250     |     error test 5.100\n",
      "epoch 12500     |     cost 5.670344e-01     |     error train 5.275     |     error test 5.175\n"
     ]
    }
   ],
   "source": [
    "num_eg = 4000\n",
    "vertex = 37\n",
    "\n",
    "inputdim = 20\n",
    "\n",
    "#train ensemble \n",
    "#data = gs.generate_ensemble([int(num_eg/2), int(num_eg/2)], vertex, [1/3, 1/2])\n",
    "data = gs.generate_ensemble_v2_label(num_eg, vertex-1)\n",
    "#data = gs.generate_ensemble_same_label(num_eg, vertex-1)\n",
    "\n",
    "\n",
    "#Normalize input:\n",
    "datagraph = data[\"graphs\"]\n",
    "for i in range(num_eg):\n",
    "    curgraph = np.linalg.matrix_power(datagraph[i,:,:],k) + np.eye(vertex-1)\n",
    "    #degree = np.sum(curgraph, axis = 0)\n",
    "    #updategraph = np.minimum(curgraph, np.ones((vertex-1,vertex-1)))\n",
    "    data[\"graphs\"][i,:,:] = curgraph \n",
    "\n",
    "data = gs.augment(data)\n",
    "for i in range(num_eg):\n",
    "    curgraph = data[\"graphs\"][i,:,:]\n",
    "    degree = np.sum(curgraph, axis = 0)\n",
    "    updategraph = 1/np.sqrt(degree) * curgraph * 1/np.sqrt(degree)\n",
    "    data[\"graphs\"][i,:,:] = updategraph\n",
    "datavertex = np.random.normal(loc=1, scale=0.0, size=(num_eg, vertex, vertexdim)).astype(np.float32)\n",
    "print(data[\"graphs\"][1,:,:])\n",
    "#data = gs.generate_ensemble_n2v(num_eg, vertex)\n",
    "#test ensemble\n",
    "testdata = gs.generate_ensemble_v2_label(num_eg, vertex-1)\n",
    "#testdata = gs.generate_ensemble([int(num_eg/2), int(num_eg/2)], vertex, [1/3, 1/2])\n",
    "#testdata = gs.generate_ensemble_same_label(num_eg, vertex-1)\n",
    "\n",
    "\n",
    "testdatagraph = testdata[\"graphs\"]\n",
    "for i in range(num_eg):\n",
    "    curgraph = np.linalg.matrix_power(testdatagraph[i,:,:],2) + np.eye(vertex-1)\n",
    "    #degree = np.sum(curgraph, axis = 0)\n",
    "    #updategraph = np.minimum(curgraph, np.ones((vertex-1,vertex-1)))\n",
    "    testdata[\"graphs\"][i,:,:] = curgraph \n",
    "testdata = gs.augment(testdata)\n",
    "for i in range(num_eg):\n",
    "    curgraph = testdata[\"graphs\"][i,:,:]\n",
    "    degree = np.sum(curgraph, axis = 0)\n",
    "    updategraph = 1/np.sqrt(degree) * curgraph * 1/np.sqrt(degree)\n",
    "    testdata[\"graphs\"][i,:,:] = updategraph \n",
    "\n",
    "testdatavertex = np.random.normal(loc=1, scale=0.0, size=(num_eg, vertex, vertexdim)).astype(np.float32)\n",
    "#testdata = gs.generate_ensemble_n2v(num_eg, vertex)\n",
    "#reformat \n",
    "data[\"graphs\"] = data[\"graphs\"].reshape((num_eg, vertex, vertex))\n",
    "testdata[\"graphs\"] = testdata[\"graphs\"].reshape((num_eg, vertex, vertex))\n",
    "    \n",
    "#reformat \n",
    "GCN.testdat = data[\"graphs\"]\n",
    "GCN.testlabeldat = data[\"labels\"]\n",
    "GCN.testvertexdat = datavertex\n",
    "gcndata = GCN.sess.run([GCN.gcn], feed_dict = {GCN.input: GCN.testdat, GCN.label: GCN.testlabeldat, GCN.vertexlabel: GCN.testvertexdat,\\\n",
    "                                                 GCN.mode: False})\n",
    "gcndata = np.asarray(gcndata)\n",
    "print(gcndata[:,0,vertex-1,:])\n",
    "\n",
    "\n",
    "data[\"graphs\"] = gcndata[:,:,vertex-1,:].reshape(num_eg, -1)\n",
    "\n",
    "GCN.testdat = testdata[\"graphs\"]\n",
    "GCN.testlabeldat = testdata[\"labels\"]\n",
    "GCN.testertexdat = testdatavertex\n",
    "gcntestdata = GCN.sess.run([GCN.gcn], feed_dict = {GCN.input: GCN.testdat, GCN.label: GCN.testlabeldat, GCN.vertexlabel: GCN.testvertexdat,\\\n",
    "                                                 GCN.mode: False})\n",
    "gcntestdata = np.asarray(gcntestdata)\n",
    "testdata[\"graphs\"] = gcntestdata[:,:,(vertex-1),:].reshape((num_eg, -1))\n",
    "\n",
    "\n",
    "\n",
    "#Fully connected model \n",
    "FullNN = FullNN(data[\"graphs\"], data[\"labels\"], testdata[\"graphs\"], testdata[\"labels\"], [inputdim,15,10,5,1])\n",
    "\n",
    "#Test Model \n",
    "#FullNN.inputdat = testdata[\"graphs\"]\n",
    "#FullNN.labeldat = testdata[\"labels\"]\n",
    "#FullNN.trainmode = False\n",
    "#FullNN.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"graphs\"][i,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
